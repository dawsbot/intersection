Intexticated: distracted by the act of texting while driving 
to such a degree that one seems intoxicated.
-	Anonymous

What follows is a partial excerpt from my new book (of "Phubbing" fame), Too Much of a Good Thing: Are You Addicted to Your Smartphone?

I have one request of everybody who reads this. Please share it with your friends on Facebook, Twitter, Instagram, and LinkedIn. It might save their life.

We all know the tell-tale signs.... a hand over the right ear of the driver in front of us, someone driving 50 miles per hour on the Interstate. Or, the slow "lane drift" as the driver in front or beside you slowly creeps into your lane and then corrects his lane tracking before he does it all over again. The "mobile" phone is no misnomer. As many as 100 million U.S. drivers admit to talking or texting while driving (1). As many as 91 percent said they talk on their cell-phone while driving and a surprisingly 50 percent confess to texting while operating a motor vehicle (2).

The debate on cell-phone use while driving comes down to our ability as humans to multitask. Can we hold conversations, check e-mails, surf the Internet, or text a friend while operating a motor vehicle? Many of us think we can but what does the data say? The National Safety Council (NSC), is a non-profit, non-governmental public service organization, whose mission is to "protect life and promote health" in the United States (3). The NSC has taken a careful look at whether humans can operate motor vehicles safely while using their cell-phones. In a brief report entitled, "The Great Multitasking Lie", I think you know where this is going, the people at NSC lay to rest the idea that we can operate 2,000 pound-plus motor vehicles without placing ourselves and others at risk. I call it "The Great Driving While Distracted Disconnect." The majority of us have acknowledged that talking and texting while driving are two of the most dangerous things we can do behind the wheel, yet 81% of drivers admit to talking or making a phone call while driving (4).

The fine folks at NSC are doing their best to debunk the multitasking lie that we can operate a motor vehicle safely while using our cell-phone. They focus their attention on four myths about multitasking that they feel "blinds" the driving public to the dangers of cell-phone use while driving.

The first myth they debunk is that Drivers Can Multitask. In fact, the very idea that humans can multitask is shot down as a common misconception. Driving a car and using a cell-phone, argues NSC, are both thinking tasks that require the involvement of many different areas of our brain. We can't do both things simultaneously, so our brain attempts to switch back and forth between each of the activities. So, any even momentary focus on our cell-phone conversation comes at the expense of lack of attention to our driving task.

The NSC uses the example of walking and chewing gum to make their point. The common thinking goes, if we can walk and chew gum at the same time, we should be able to use our cell-phone while driving. This analogy, however, is a poor one. Walking is a thinking task but chewing gum is a non-thinking task.

This reminds me of a funny story about my daughter Chloe' when she was a child - maybe six or seven years old. Her mother warned her that it probably wasn't a good idea to read a book while taking our nightly walk. But, as kids will do, she insisted and promptly tripped on the curb and skinned her knee--so much for multitasking.

A second myth is that talking to someone on your cell-phone while driving is no different than talking to a passenger in the car. Wrong again. The NSC cites a 2008 study out of the University of Utah that found that drivers talking on their cell-phone are "more oblivious" to constantly changing traffic conditions because the person on the other end of "the line" (old habits die hard) have no idea of the current traffic conditions currently being encountered.

On the other hand, a passenger is experiencing the same road conditions as the driver and acts as an "extra set of eyes and ears" regarding the driving situation. A conscientious passenger may quit talking when the driving situation warrants greater concentration or even provide advice or warnings about various traffic developments. The conversant on the other end of the line is blissfully unaware of any such developments and can only act as a distraction to the driver.

A third myth debunked by NSC is that "hands-free" devices will solve any problems created by driver inattention while using their cell-phone. Oh, if this were only the case. Whether it's hand-held or hands-free the conversation remains a distraction because your brain is still attempting to deal with two thinking tasks (driving and talking) simultaneously. To support their contention, the NSC cites a study out of Carnegie Mellon University, that found activity in your brain's parietal lube "that processes movement of usual images and is important for safe driving, decreases by as much as 37% when listening to language..." (5). Referred to as "inattention blindness", drivers talking on their cell-phone miss as much as 50% of their driving landscape including stop signs, pedestrians, or on-coming traffic.

The fourth myth debunked by NSC is a big one, but still kind of a lame excuse by cell-phone drivers. The excuse goes something like this, "well, I may have lost some reaction time while talking on my phone, but it's better than drunk driving." Sad, I know, but astonishingly not true. A study at the University of Utah found that drivers using their cell-phone actually had slower reaction times than drivers who were legally drunk (.08 blood-alcohol content). That's hard to believe, our friend who had a few too many drinks at Applebee's after work is less of a menace than those of us who text to check on "what's for dinner" on our way home from work.

To learn more about how our love affair with smartphones is affecting our lives go to www.smartphoneloveaffair.com.The successes of Silicon Valley are well-chronicled -- some of our brightest minds take a singular idea and turn it into a billion-dollar company that changes the world. Every venture capitalist across the United States aims to be the one who discovers the next Facebook, Snapchat, or Uber, so naturally, the first step is to try and emulate what has already worked so well in regards to business models and investment structures.

In these circumstances however, it's not just the positive attributes that often come with Silicon Valley's sought-after models for investment success. Some of the negative aspects of the dealings within Silicon Valley inevitably also end up being replicated -- most unintentionally. One such unintended consequence might be the permeation of practices that make it more difficult for women and minorities to compete far beyond the Northern California zip codes that make up the Valley, due to its own well-chronicled struggles with diversity.

While it's disappointing that race and gender continue to preclude talented and innovative individuals from accessing the same opportunities as others, the real concern is that this issue doesn't begin and end in Silicon Valley as many people think. It travels in tandem with Silicon Valley's influence in business across America.

This is particularly true when it comes to startups seeking funds to jumpstart their business. A venture investor's goal is to minimize their risks while maximizing their potential for rewards. Therefore, they like to put their money into what they know, people they are familiar with, and business models they find comfort in. This was well-chronicled in a recent TechCrunch piece that detailed the various ways venture capitalists succumb to bias.

The Silicon Valley approach is most often to quickly sort through those that don't almost perfectly align with your model of success to find those same, comfortable scenarios many are used to -- alumni from their college, former employees of big name companies, or mutual connections. The end result is the perceived risk vs. reward profiles that are biased toward what these investors already know.

Given the volume of proposals venture funds must sort and filter through, one can imagine the number of innovative and potentially profitable business plans that are never invested in because they don't fit into the mold of commonality. An unintentional consequence of this approach is proving to be a bias against individuals who do not fit into a precast mold, which inherently has some VCs seeing greater risk than reward.

I'm not suggesting that the majority of Silicon Valley is intentionally discriminating against certain genders or races, but the numbers are startling -- a report from 2010 stated 87 percent of startup founders that received VC funds were white, while only one percent were African American. There is something to be said about unintentional cognitive biases, as outlined by the TechCrunch piece, and how they are affecting the way VCs see potential risk of change versus investing in the comfort of what they know.

Cognitive bias refers to a systematic pattern of deviation from norm or rationality in judgment, whereby inferences about other people and situations may be drawn in an illogical fashion. Flaws in perception of what is high risk vs. high reward can lead to blind spots in decision making that generate bias toward some business models and away from others, and for some people and away from others. This leads to both racial and gender bias as well. So it's logical that there is too much comfort with the model that is already in place, which generally involves a young, Caucasian or Asian male leading a company that goes on to be entrusted with hundreds of millions of dollars - funds that often originate from public coffers via pension funds.

By far the largest source of capital for most venture funds is state run pension funds. So if anyone could affect change, one would assume that it would be the individual states themselves, by breaking through the perceived risks and providing a platform of equal opportunity to all. While it is fairly common to see states and cities across the country striving to ensure that all of their citizens, including their underserved minority groups, have equal access to the basic life necessities such as housing, education, and now healthcare, it might be time that they also consider affording equal access to venture capital and other investment resources.

When discussing inequalities and the lack of diversity in Silicon Valley, it has become far too common to call for the large companies in the area to instill better hiring practices to onboard more minorities and women. This obviously would be a positive step in the right direction, however there has to be a broader consideration given to the wider issues at hand that have sprawled outside of California and across the nation and likely beyond.Companies could face action from European privacy regulators if the European Commission and United States do not come up with a new system enabling them to shuffle data across the Atlantic in three months, the regulators said on Friday.

The highest EU court last week struck down a system known as Safe Harbour used by over 4,000 firms to transfer personal data to the United States, leaving companies without alternatives scrambling to put new legal measures in place to ensure everyday business could continue.

Under EU data protection law, companies cannot transfer EU citizens' personal data to countries outside the EU deemed to have insufficient privacy safeguards, of which the United States is one.

EU data protection authorities meeting in Brussels to assess the implications of the ruling, said in a statement that they would assess the impact of the judgment on other data transfer systems, such as binding corporate rules and model clauses between companies.

"If by the end of January 2016, no appropriate solution is found with the U.S. authorities and depending on the assessment of the transfer tools by the Working Party, EU data protection authorities are committed to take all necessary and appropriate actions, which may include coordinated enforcement actions," the watchdogs said in a statement.

The Commission and the United States have been in talks for two years to reform Safe Harbour after former U.S. National Security Agency contractor Edward Snowden revealed the existence of mass U.S. government surveillance programs.

Talks have been hampered by the difficulty of extracting sufficient guarantees that U.S. authorities' access to personal data would be limited and proportionate.

The regulators said in their statement the EU and the United States should negotiate an "intergovernmental agreement" providing stronger privacy guarantees to EU citizens, including oversight on government access to data and legal redress mechanisms.

Multinationals can set up internal privacy rules which have to be approved by regulators to transfer data to the United States, known as binding corporate rules. However, only about 70 companies currently use this system.

Lawyers have said alternative data transfer systems could also be at risk to legal challenge since they do not provide stronger protection against U.S. government snooping than Safe Harbour did.

"The good news is that the European data protection authorities have agreed on a kind of grace period until the end of January," said Monika Kuschewsky, a lawyer at Covington & Burling.

Usually, once in our lives, if not more than once, there's a sentence that flutters out of our mouths without a hesitation. People have uttered this sentence in all cases of need, where they wanted something really badly but they felt as if that new book would enhance their lives or that video game would make the winter bearable or this movie would help with bringing the family together. So people say it, feeling like they really mean what they say,

It's the sentence, "I need that."

A dictionary has many definitions for the word "need". Until October, when I decided to take up a challenge by a friend to not use any Internet for a whole month, I didn't really understand what I needed or even why I needed it.

In September 2014, the sun speckled the ground with bursts of bright light even though the weather was cold in Chicago. I was sitting in a park with an engineer friend of mine, stealing his fries as we talked about the internship that I applied for but didn't make. The topic nestled into the internship miss until, suddenly, he blurted out an exclamation of "oh my god, Robbie, you have GOT to read this!"

And so I did, or rather, listened, having limited vision and everything. It was an article that said that killing net neutrality would help the disabled. Verizon was saying that, if the Internet were split into a fast lane and a slow lane, disabled people would have much better Internet. Naturally, the irony wasn't lost on me. In most cases, no matter how politically correct people wanted me to be ever since I started saying it, a good portion of the disabled populace were very poor, so the idea Verizon had was just utter nonsense.

"That's a complete fallacy!" I spluttered, shifting my weight so my good eye could stare at Marcus full on in the face. "That's just plain wrong!"

"I know," he agreed, but we ranted and raved for a bit, just to make sure our thoughts were out in the open. Suddenly, though, as I was stealing a fry, he commented, "I have an idea. Why don't I give you a challenge, you know, like a dare?" I liked the prospect of a dare so I accepted his challenge before having the intellect to ask what it was.

"Why don't you, as a disabled person, live without the Internet for one month, and this means using Internet in schools and in libraries and the like, don't use the Internet at all for one month."

And that's how it began.

Now, my memoir details my journey of living offline. Through my words and reflections, readers will know what adapting to a new kind of world is like. I was soon swept up in a different world, a world that was inaccessible to me and a world that I had to learn how adapt to, on my own since I live in an apartment complex by myself. I really did learn the difference between needing something and wanting the convenience of something.

I assumed I was going to do the everyday things that people did, such as walk outside, even though it was starting to get cold in Chicago. I thought my entries were going to be filled with sentences outlining what I did, rather than what I'd think about and declarations and observations of people. I thought that I'd write more about what I did and why I did it, rather than my observations about the Internet-less life and how it changing everything from communication to education to human interaction.

In the memoir, many aspects of life are examined. Why? Because I had a lot more free time to share with myself. The memoir is a diary that's intimate and allows for a glimpse into the human psyche before being connected to everyone.

Living offline changed me in many ways that I didn't even see coming. For the first few days, I needed to get online, I wanted to look up something. I wanted to type in the commands and the search strings that would get me exactly what I wanted, how I wanted it, where I wanted it. Without that power, for a few days, I was utterly lost because I didn't know how to cope after that power had been taken away.

While I felt as though I was going to back down on the first few days, I gave it a shot still, and kept on with the challenge.

The fact is, the Internet is a requirement, especially for the disabled. I experienced much frustration simply because I could no longer do something so basic, such as hooking up a landline phone because I couldn't download the manual from the website. I had to rely on the sighted population more than I have ever needed to.

This is because there isn't as much accessible information offline as there is on the Internet. On the Internet I can look up any news I want to look up or any manual, for that matter. Take news content: writers are not filtered by space and advertising columns so they can pepper the Web page with in-depth reporting and I could read it all.

Mainstream offline media doesn't tell you about all the news that's happening or the kind of topics people want to know. The fact is, people want to know. On the radio and TV everything is delegated by space and time. When you have limited options to get information, information becomes a need, not a luxury. I had to cope with losing that by asking more questions from other people and relying on their answers. Sometimes, it was effective. Other times, it left me feeling as if I was being denied information simply because I didn't use the Internet.

We live, however, in a world that needs the Internet. I learned that the hard way when I didn't get hired for a job because I couldn't use the Internet. It really has shocked me how it's turned into an unclassified utility. Sure, apartment owners are saying it is a utility but not the government, not the people higher up. It should be. Why? Because I know what it's like, as a disabled person, to live without the Internet for one month. A disabled life without Internet is not a completely independent world. The Internet breaks down barriers, even if we can't see them.When my kids were young, making things involved macaroni, feathers, glue and paper plates. The desire to make things and the joy that comes with completion has not diminished. But boy have the tools changed.

Drones, robots, 3D printers, electronic kits and tech infused building blocks have become the new spirit of project time with the kids. And "I-made-it-myself" play is taking its place at the top of heap in the hopes of fostering the skills the 21st century job market will require.

This message hit home to me on our annual pilgrimage to the NYC Maker Faire a few weeks ago. Maker Faire is sort of a mashup between a family picnic, the world's biggest science fair, and a geek playground. This year's Faire featured 900 makers and 90,000 visitors who trekked to The New York Hall of Science (NYSCI) in Queens, New York, to get a fix on the triumphs and foibles of making stuff.

What started as sort of a Whole Earth Catalog counter culture mentality in the late 60's has been resurrected, resuscitated and reborn as the Maker Movement, where self-sufficiency, ingenuity and a spirit of intrepid exploration abound. Here are a few things that struck me as I watched tens of thousands of people gather to celebrate making.

It's A Family Affair. One look at the photos in the slideshow below and it's plain to see that kids were at the forefront of the Faire. Geek parents stood proudly, encouraging their budding young makers. Over 350 of the exhibitors at the show had hands-on activities for families -- from wiring circuit boards to playing a synthesizer, from painting with pixels to drone flying. Nearly every kid had some "made" contraption to show off by the end of the day and me, I wanted to build a "rent-a-kid" booth to help those left out of the fun.

A Low Bar to Entry. Easy to master, do it yourself kits from companies like littleBits, Lego Mindstorms, MakeWonder, and Spin Master are giving kids a challenging but manageable sort of paint-by-numbers version of making stuff. By creating simple games, companies like these invite kids to try to get something to work, a robot that dances or lights blink on and off. While not inexpensive (hovering at about $100 each), the experiences toys like this provide can get increasingly intricate as kids skills grow.

It's Not Only About High Tech. Cardboard and fabric also played their roles at the Faire. Coke Zero and Mentos mixed together to create a Fizzology experience. Non tech projects ranged from storytelling to knitting.

Dropping Prices. The prices of 3D printers will be as low as $400 by this holiday season. Powerful robots are selling for $100 and change. Basic drones are now under $100. These are the modern day Erector Sets and Lincoln Logs, with a splash of high tech.

The School Tech Connection. Schools are jumping on the bandwagon, purchasing 3D printers and soldering irons for collaborative curriculum based products. Programs like SciTech Kids, RoboFun and Brooklyn Robot Foundry were just a few of the organizations at the Faire advertising afterschool and weekend classes to build robots, fashions, and video games. School teams from all over the tri-state region displayed their creations, many the products of after school clubs and science classes.

Entrepreneurship Starts Here. Dozens of under 18 year olds were displaying their wares. Becky Stern of Adafruit was showing young (mostly women) how to sew with circuitry to create fashions. ToyLabCo's 3D dolls were printed to look like everyone from Marissa Mayer to Carly Fiorina. After the show, I spoke to one young maker, Daniel Luntzel, from Colgate College where they've started a "Build Your Own 3D Printer" project to provide 3D printer kits to students.

Applied Science. It's not pure love of science, rather science in the context of solving real problems that gets these kids pumped. Building your own circuit board is cool. Building it to light up a doll house or a clubhouse is cooler. At the Faire, kids (and a few anxious adults) lined up for soldering lessons. Even chilling out can become a maker activity at the Faire when Netflix gave out plans for a Chill Button to automatically dim the lights and turn on your show.

The Face of Engagement. But what we love best are the faces at Maker Faire. Young (and young at heart), curious and optimistic, here are the faces of the Maker Faire.These days, even hackers will nickel and dime you.

That's according to a report released Thursday by Intel Security Group's McAfee Labs, which found login credentials to an online video streaming website can be bought for as little as 55 cents -- slightly more than the cost of a postage stamp. (Credentials for premium cable streaming like HBO Go, meanwhile, sell for a comparatively steep $7.50.)

The report investigated the murky world of buying and selling stolen digital information online, where everything from financial data and online services to a person's entire digital identity can be had -- for a price. 

According to researchers who monitored the various websites, chat rooms and communities hosting these transactions, stolen credit card info goes for $5 to $8 for a basic U.S. card number, and $25 to $30 for a European Union card number:


MCAFEE LABS
Buyers can pay extra for access to the cardholder's full name, billing address, expiration date, PIN number, Social Security number, mother’s maiden name, date of birth, and CVV2.

"A criminal in possession of the digital equivalent of the physical card can make purchases or withdrawals until the victim contacts the card issuer and challenge the charges,” Raj Samani, chief technology officer of Intel Security for Europe, the Middle East and Africa, said in a statement. “Provide that criminal with extensive personal information which can be used to ‘verify’ the identity of a card holder, or worse yet allow the thief to access the account and change the information, and the potential for extensive financial harm goes up dramatically for the individual.”

Bank account logins, meanwhile, can be had for a cost of $20 to several hundred dollars, depending on the (supposed) account balance:


MCAFEE LABS
Researchers found entire digital identities could be purchased, potentially allowing someone to take over an innocent person's email, financial accounts and social media accounts, effectively hijacking a person's identity.

In one particularly troubling instance, a criminal claimed to have access to a French hydroelectric generator and was offering access to the critical piece of infrastructure online. The hacker provided screenshots of the generator's internal software as proof, attempting to convince wary would-be buyers.

“Like any unregulated, efficient economy, the cybercrime ecosystem has quickly evolved to deliver many tools and services to anyone aspiring to criminal behavior,” Samani said. “This ‘cybercrime-as-a-service’ marketplace has been a primary driver for the explosion in the size, frequency, and severity of cyber attacks. The same can be said for the proliferation of business models established to sell stolen data and make cybercrime pay.”


Motivation

When I first started to learn how to create and consume API’s, I was lost. I jumped through three to four tutorials before I had someone sit down and teach me.

My aim in this tutorial is to teach people with some technical experience how to build an API and how to consume data from other APIs.

Prerequisites:

1) node installed

2) running on linux or mac. Let’s be real people, if we’re not playing games, we don’t need Windows.

3) some knowledge of javascript

Agenda:

What is an API?

What is REST?

CODE TIME!

What is an API?

API is an acronym for “application program interface”. This is just fancy talk for how two software components interact. Programmers create “building blocks”, an API, to make it easier for other programmers to interact with their database or hardware. This could be how a programmer interacts with the hardware to create a nice graphical user interface. It could be how Twitter exposes their database to make cool applications!

tl;dr:

An interface for interacting with databases or hardware
What is REST?

Representational State Transfer. Yeah, ignore that. REST is the most popular style of communication for web applications. It allows us to communicate via URI and HTTP (think of URI as a URL, like: http://alexcampbell.co). That’s all we need to know for now, let’s jump into code!

tl;dr:

A way to communicate via URI and HTTP
Our Tools

For this application we’ll be using Node.js and Express.js. Not because it’s better than anything else, but because I’m most comfortable using it.

Node.js is a platform for building network applications (servers).

Express.js is a package that makes using node.js easier.

Part 1: Creating our own API

In this part, we’re going to create our own server. We’re going to create our own server that is an API that someone else could use. It’s not going to do much, but damnit, it’s an API nonetheless.

Server.js, our first file

Libraries:

Let’s create our barebones server. Create a file called server.js:

//server.js


// Include the packages we need
var express     = require('express'),
    http        = require('http'),
    body_parser = require('body-parser');
Okay so this first few lines we’re just including the libraries we need in order to run our application.

We include express so that we we can use all of the functions that the framework includes.
We include http so that we can create our server using HTTP. This will allow us to access it via URL!
We include body_parser so that we can do POST requests.
The basics, setting up our express app:

//server.js


// Include the packages we need
var express     = require('express'),
    http        = require('http'),
    body_parser = require('body-parser');

var app = express(); // Create our app using express


//Create our app using body parser
// This will allow us to get data from a POST
app.use(body_parser.urlencoded({ extended: true })); 

// Set our port
app.set('port',3000);
The first thing we did was create a variable called app that is equal to express. This means that we now have an application that can utulize the express framework.

Next we tell our app to use body_parser. Don’t worry about the details for now, just know that it allows us to do POST requests.

Finally we set ‘port’ to be equal to 3000 for our application. This means that when we run our server it will be available at http://localhost:3000. Localhost is just your computer, so for now only you will be able to see it!

tl;dr:

Create our app using express, use body parser to allow POST requests, and set the application’s port to 3000 so we can access it at http://localhost:3000.
Let’s create our basic server:

//server.js


// Include the packages we need
var express     = require('express'),
    http        = require('http'),
    body_parser = require('body-parser');

var app = express(); // Create our app using express


//Create our app using body parser
// This will allow us to get data from a POST
app.use(body_parser.urlencoded({ extended: true })); 

// Set our port
app.set('port', 3000);

// Create our server using the app
var server = http.createServer(app);


// Our server will listen on port 3000 and log out when it works!
server.listen(app.get('port'), function(){
  console.log("Listening on port "+ app.get('port'));
})
We create a variable called server that is an http server using our express app that we just created. Finally we tell our server to listen on the port we just created, 3000. We then tell it to print out “Listening on port ". Simple enough, right?

tl;dr:

Create a server called ‘server’ that uses http to create a server using the options we set on our app
Callback function?

Although let’s back up a second. Do you see how when we created the server we had this funky function as our second parameter? This is called a callback function. All that this is saying is the following: “Hey server, listen on port 3000 (app.get(‘port’), and when you’re done with that print out that you’re listening”. We’ll see a few more callback functions when we get into routes, but for now just think of it as the thing that gets done after the first function finishes.

tl;dr:

“Hey server, listen on port 3000 (app.get(‘port’), and when you’re done with that print out that you’re listening”.
Try it out!

BOOM! You just created a server. Hell yeah, nice job. Let’s boot it up. Go into the directory where your server.js lives and type the command node server.js.

Sorry that was mean, I knew it was going to break. You should have an error that says something like: “Error: Cannot find module ‘express’”. But wait we included it in our server.js file, why doesn’t it work?!

Fixing it:

Yes your app sees it, but the computer you’re running on doesn’t have it installed. More specifically, the directory you’re in (where your app lives) doesn’t have it installed. We could manually install all of the dependencies, but we’re developers, we don’t like to do things twice. Let’s create a package.json file. We can think of this file as the instruction manual for our application. It tells it what it needs to install to run.

tl;dr:

The libraries are not installed in your app. We need to add a package.json file.
package.json

{
  "name": "MyFirstAPI",
  "version": "0.0.1",
  "main": "server.js",
  "author":"Alex Campbell",

  "dependencies":{
    "express": "^4.7.4",
    "body-parser": "~1.5.2"
    }
}
Create this file in the same directory as your server.js file. Notice I added a few things here. The name of my application, the version, where our main file is, and who wrote it. This stuff is just to keep track of data associated with the application. The interesting stuff is below in dependencies. We include express and body-parser, good! Although we notice that http is not there. This is because it is built in to node.js. So when we install node.js, we install http!

Now in order to run this file just type ```npm install`` in the directory that the package.json is.

Woohoo we installed stuff. Well done.

tl;dr:

Add meta-data and dependencies to the file. We don’t need HTTP because it’s included in node.js
Try, try again:

Let’s try to run our app again: node server.js Cool it’s listening on port 3000! Quick, go to your browser and put http://localhost:3000 into the url bar.

Cannot GET / ? What the hell man, I thought it would work. Well it is working, you created a server. Although the server doesn’t know what to do at the URL http://localhost:3000/. If you go to http://localhost:3000/test you’ll get a similar error. This is where routing comes into play, which is the next part of our tutorial.

tl;dr:

The server is running, but we haven’t written any routes yet.
Let’s create some routes:

You guys are gunna’ be impressed how easy it is to create an API. Let’s add 3 lines of code:

//server.js


// Include the packages we need
var express     = require('express'),
    http        = require('http'),
    body_parser = require('body-parser');

var app = express(); // Create our app using express


//Create our app using body parser
// This will allow us to get data from a POST
app.use(body_parser.urlencoded({ extended: true })); 

// Set our port
app.set('port', 3000);

// Create our server using the app
var server = http.createServer(app);


// Our server will listen on port 3000 and log out when it works!
server.listen(app.get('port'), function(){
  console.log("Listening on port "+ app.get('port'));
})


  app.get('/', function(request,response){
    response.send("I just created an API, I am a god.");
  });
Woah don’t get too hasty, let’s understand what’s going on here. We’re telling our app the following: “When someone hits the url http://localhost:3000/, send the string over”.

Although it’s a bit more complex than that. We see that callback thing again here. The reason we have this callback this is because our app.get is asynchronous. In other words, while app.get is running something else on the server might be running. We give it a callback function to tell it what to do when it’s done. But this time it has two parameters: request and response. These are objects that represent data that our app is sending out and the data that our app is receiving. So when we say response.send(“I just created an API, I am a god”), it’s saying: “Add this string to the response object’s ‘send’ field.”

If you haven’t already, go to http://localhost:3000/ and bask in your glory.

tl;dr:

Add the root route (‘/’) and when someone accesses that URL send back a string.
Part 2: Using someone else’s API:

In this section we are going to utulize an API that converts any string into a ‘yoda string’. It will take a string and return how yoda might say it. Or, Take a string and return how yoda might say it, it will. Yes, hmmm. Let’s do eet!

Let’s hit another API!

Alright let’s make it do something cool. Let’s use an API to translate anything we say into yoda speak. I’m about to hit y’all with a lot of unfamiliar code, don’t freak out:

//server.js


// Include the packages we need
var express     = require('express'),
    http        = require('http'),
    body_parser = require('body-parser'),
    request     = require('request');

var app = express(); // Create our app using express


//Create our app using body parser
// This will allow us to get data from a POST
app.use(body_parser.urlencoded({ extended: true })); 

// Set our port
app.set('port', 3000);

// Create our server using the app
var server = http.createServer(app);


// Our server will listen on port 3000 and log out when it works!
server.listen(app.get('port'), function(){
  console.log("Listening on port "+ app.get('port'));
})

  // When we hit http://localhost:3000/ send over this string.
  app.get('/', function(req,res){
    res.send("I just created an API, I am a god.");
  });

  // When we hit "http://localhost:3000/translate?sentence= some sentence" it will translate our string into yoda speak!
  app.get('/translate', function(req,res){
    request({
      url: "https://yoda.p.mashape.com/yoda",
      qs: {sentence: req.query.sentence},
      headers: {
        "X-Mashape-Key":"W0XfNKn3JJmsh0Pq0bsfmunVYrOvp1BSVkLjsn6IP3xd0N2ChB",
        "Accept": "text/plain"
      },
    },
      function(err,response,body){
        if(!err){
          res.send(body);
        }
      });
  });
Use the request library to hit third party APIs

First things first, we’re using a new library called Request which allows us to easily hit third party APIs. So let’s add it to our package.json: request”: “~2.37.0” in the dependencies section. Your package.json should now look like this:

{
  "name": "MyFirstAPI",
  "version": "0.0.1",
  "main": "server/server.js",
  "author":"Alex Campbell",

  "dependencies":{
    "express": "^4.7.4",
    "body-parser": "~1.5.2",
    "request": "~2.37.0"
    }
}
Let’s run npm install again to update our libraries.

Now let’s walk through this ‘/translate’ route step by step:

1) we add request to the packages we need.

2) we will use request to make an API call to a yoda translator. This API exists at the URL “https://yoda.p.mashape.com/yoda” so we set our url parameter to be that.

3) we need to set a query string. This is going to be the sentence that is sent to the API to be yodafied (yeah I just made that word up). We will name the query string sentence so that we can make requests like: “https://yoda.p.mashape.com/yoda?sentence=I%20love%20lumpy%20space”. If we changed:

Note: The “%20” is just a space, so go ahead and use spaces in the URL bar if you want!
qs: {"sentence": req.query.sentence}
TO

qs: {"thing": req.query.thing}
then we would make our request like this: “https://yoda.p.mashape.com/yoda?thing=I%20love%20lumpy%20space”.

4) req.query.sentence? What the hell is that? We’re telling our app on request to look at the query string. Within that querystring give us the value of sentence. This is whatever you typed into your URL!

5) Next we need to add the headers. This is extra data associated with our request that tells it how to act, or passes along authentication information. Since we’re using mashape, their APIs expect an “X-MASHAPE-KEY”, so we pass it along in the headers. Best practice, we would not expose our API key, but for the scope of this tutorial, don’t worry about it. We also tell it to Accept text/plain. This means that our API should only accept plain text.

6) After we’ve passed all this information along, we have a callback function. So once the request is made, it will execute this function. Similar to our “http://localhost:300/” route, we send a string across the server. This time it’s the body of the response.

